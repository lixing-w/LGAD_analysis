This project analyzes IV and CV scans of LGADs.

Workflow and Purpose:
    We want to study how environmental conditions might affect performance 
    metrics of LGAD sensors. We primarily focus on IV scans, which tell 
    us the breakdown voltage, depletion voltage, and importantly how the 
    leakage current behaves under different reverse bias voltages.

    Analysis methods include non-ML methods and ML methods.

    Non-ML methods involve fitting a line using customized algorithms to 
    the linear region of an IV curve, to extract breakdown voltage and 
    estimate depletion voltage. We also try to estimate depletion based on 
    CV scans, if they are provided. See analyze.py

    ML methods involve several pathways.
    1)  Map directly the environment vars to the whole IV curve. Then we can use 
        non-ML methods to extract performance metrics from generated curves. 
        This involves a baseline MLP model (sensor-specific) in model.py and 
        train.py
       
    2)  Extract key ML features from IV curves, then examine correlation between
        the environment vars and these features. Finally, mapping the 
        correlation between the ML features and the IV curve itself would 
        tell us how the curve behaves in relation to the env vars. 
        This involves:
        -   a "semi-supervised" Autoencoder that learns to encode IV curve -> 
            latent vector, then decode (reconstruct) IV curve from the latent. 
            (Note: for some noisy measurements, the reconstruction can even 
            serve as a smart denoise algorithm!) The encoder and the decoder can 
            be used separately to go between the IV space and the latent space. 
            Check model.py, and train_autoencoder.py.
        -   a simple MLP that predicts latent (generated by the autoencoder) 
            based on env vars. See model.py and train_env_to_latent.py
        -   additional visualization and evaluation tools to examine what 
            latents represent. See visualize_latent.py, and explain() 
            in train_env_to_latent.py.
        -   additional methods to correlate latents with specific IV curve 
            behaviors. (underdeveloped)

    3)  Map directly the env vars to interested performance metrics. This part 
        is still underdeveloped, because it relies heavily on metrics extracted 
        by non-ML methods, which are not robust for now. And we don't have 
        a lot of data.

    The primary method I'm using is autoencoders.

    Using autoencoders gives us the advantage of preserving more useful and 
    detailed information from IV curves to construct correlation. They 
    extract whatever information essential to the curves, this is more and 
    more robust information than what non-ML methods can provide. More 
    importantly, autoencoders can extract information from solely the curve 
    itself, regardless whether env var labelings are missing. 
    For example, a lot scans' humidity are missing. These missing labels are 
    a challenged to pathways 1 and 3, as they typically expect "uniform" 
    input types. Another advantage is that if in the future, we want to study 
    other env vars that are not included now, we can simply run the correlation
    analysis with the new set of vars and the already learned latents. Latents 
    and the autoencoder don't need to be changed at all. The correlation between 
    latent and the IV curve itself don't need to be changed as well, because 
    they are independent of the env var labels, so we can expand research 
    conveniently in the future. Finally, autoencoders can convert variable
    -length IV sequences to a fixed-length latent vector, which is convenient 
    for analysis. 

    However, as of now, the autoencoder has a simple architecture, and 
    explanability of the network and the latent may not be promising. I tried 
    to make the latent space more interpreble by using a regressor, which 
    predicts env var labels based on latent, in training, to nudge the model 
    encode information in a more physically meaningful way. The effectiveness 
    of such nudge is subject to evaluation. Further, the autoencoder is 1D-CNN 
    based, which can only handle fix-length sequences. Hence the database is 
    padded to the longest sequence by adding trailing 0. If the longest length 
    changes because we obtained more scans or something, we must either truncate 
    the sequence, or train a different model. 
    
    We don't have too many IV scans to learn (e.g. only around 402 good scans 
    in ./data/ivcvscans); we lack information on humidity and measurement 
    duration of many scans; 
    we lack information about the architecture and parameters (such as doping 
    concentration) of the LGADs themselves. Some changes were made to some same 
    LGAD between scans, and we have no good way of quantifying these changes 
    other than knowing the scans took place on different dates.
    The data is not ideal, still, we will see what correlations we can
    explore between environment variables and IV behaviors.

    The autoencoder has a latent dimension of 18 for now. Generally, larger 
    dimension means better reconstruction, but smaller dimension means 
    better information extraction and less risks of overfitting. After 
    experimentation, we found that any number lower than 16 caused the model to 
    reconstruct the correct shape but the curves were shifted in some region for 
    many curves.  For some curves (probably due to uneven distribution of data), 
    the model still cannot reconstruct accurately with 18 dims, but 18 is a large 
    enough number for latent dimension. Future work on improving reconstruction 
    could focus on enhancing network architecture, training methods, and 
    augmenting the data.

Structure of the project:
    Scripts:
    analyze.py    - script for analysis and plotting with non-ML methods
    utils.py      - lib containing constants, useful functions, and Sensor class
    dataset.py    - defines several datasets for ML
    preprocess.py - lib containing functions to prepare the data for ML
    model.py      - defines pytorch models
    train_autoencoder.py - trains autoencoder to compress IV curve
    visualize_latent.py  - visualize the latent space w/ environmental variables
    train_env_to_latent.py - trains an MLP to predict latent from env vars
    train.py      - trains a sensor-specific MLP to predict IV curve from 
                    temperature, a baseline model separate from autoencoders

    Folders:
    archive_code  - some old code created before Summer 2025, not used for now
    autoencoder_model - stores trained autoencoders
    model         - stores trained MLP for pathway 1
    env_to_latent_model - stores trained MLP for pathway 2
    data          - contains all databases
    miscellaneous - miscellaneous files and data

Running non-ML analysis:
    1)  Specify the root directory to target database in utils.py, by setting
        DATABASE_DIR. This constant is share across many scripts of the system. 
        If you want to perform any operation on data in a new database, always 
        remeber to change DATABASE_DIR before running the code. Not doing so 
        might lead to undefined behavior.

        All databases should be in folder ./data at the project 
        root. Each database should contain sensor folders, each of which 
        contains scans or folders of scans. All sensor folders are assumed to be 
        titled by the sensors' name.

        Example 1:
            DATABASE_DIR 
                -> Sensor 1
                    -> Date 1
                        -> IV or CV scans...
                    -> Date 2
                        -> IV or CV scans...
                -> Sensor 2
                ...
        Example 2:
            DATABASE_DIR
                -> Sensor 1
                    -> IV or CV scans..
                ...

    2)  To start with, one can run the following command:
            python3 analyze.py --iv 
        which uses a modified version of RANSAC and other non-ML methods 
        to analyze all available IV scans in the database, estimating 
        breakdown voltages and depletion voltages, and plotting related figures. 
        Type the following command for help:
            python3 analyze.py --help

    3)  View plots generated in your sensor folders.
    4)  Further configure your analysis with data_config.txt and sensor_config.txt
        at your database root.

Database Management:
    Database management works for mainly non-ML analysis, but some features 
    supported for ML analysis as well. At the root of each database, there 
    are two files, data_config.txt and sensor_config.txt, which you can modify 
    to manage data.
    
    sensor_config.txt is where you check data points that analyze.py 
    extracted and set sensor-specific preference for non-ML analysis. 

    data_config.txt is where you tell the entire system to ignore some 
    scan (but not delete it), ignore some parts of a scan (maybe cuz it's 
    broken), and override related parameters of a scan. If a scan is fully 
    ignored, it is ignored for both non-ML and ML analysis. Being partially 
    ignored and parameter override only affects non-ML analysis. To manage, 
    we use a dedicated script language.

    Format:
        [command] [argument1:value1] [argument2:value2] ...
        One expression per line.
        Comments can be added anywhere after a '#'.
    Keywords: 
        1. DR...            - ignore specified scans
        2. SPEC...SET...    - specify scans, and set their attributes
        3. ~                - indicate range, used with temperatures and dates
            a) X~Y          - between X and Y (inclusive)
            b) X~           - at least X 
            c) ~X           - at most X
            d) X            - exactly X
    Supported arguments: 
        1. N - name of the sensor, must be specified in EVERY expression
        2. T - temperature
        3. R - regex pattern string
        4. D - date, in dd/mm/yyyy
        5. F - relative path to scan
        6. DEP - voltage after which a line is fit (for IV scan only)
        7. RT - ramp type, either -1 (down), 1 (up), 0 (none)
        Note: All except DEP can be used after DR and SPEC. Only DEP and 
        RT can be used after SET (for now).
        8. MAX - voltage after which data is completely ignored (CV scan only)
    Notes:
        The expressions are parsed top-down, and the first matching expression
        matches. So you may need to put strict expressions above looser
        ones.
    Examples:
        DR N:AC_W3096 T:~-20 
            => ignore AC_W3096 scans whose temperatures are at most -20C
        DR N:DC_W3045 T:100~120 
            => ignore DC_W3045 scans whose temperatures are between 100C 
            and 120C inclusive
        DR N:AC_W3096 T:-20
            => ignore AC_W3096 scans whose temperature is exactly -20C
        DR N:DC_W3045 T:~-20 RT:-1 D:27/10/2023 
            => ignore DC_W3045 scans whose temperatures are at most -20C, 
            AND ramp type is down, AND date is exactly on 27/10/2023
        DR N:DC_W3045 R:RoomTemp D:27/10/2023~28/10/2023
            => ignore DC_W3045 scans whose file name (at least partially)
            matches with regex "RoomTemp", AND whose dates are between 
            27/10/2023 and 28/10/2023 inclusive
        SPEC N:HPK_LGAD_3_1_6 D:16/07/2021 SET DEP:110 
            => when analyzing HPK_LGAD_3_1_6 scans on date 16/07/2021, 
            set depletion voltage to 110V to ignore bad IV scan data at 
            low voltages
        SPEC N:HPK_LGAD_3_1_6 D:16/07/2021 SET DEP:110 RT:-1
            => when analyzing HPK_LGAD_3_1_6 scans on date 16/07/2021, 
            set depletion voltage to 110V, AND set ramp type to down, 
            regardless the original ramp type.
            => ramp type needs not to be specified if it is given in and 
            automatically parsed from the scan file.

TODO:
    analyze.py:
        1. Implement analyze_file_cv(), a function that analyzes 
           CV scans specified by paths, supporting --file used with --cv.
        2. Fix plot_humidity_scans(). This function is from 
           older versions of the code and has not been updated yet.
        3. Fix find_threshold(). For a given sensor, this function 
           finds the bd_thresh, used to determine breakdown in ransac(), that minimizes 
           average uncertainty of estimation. Since higher bd_thresh leads to 
           higher estimated breakdown and vice versa, we decided to use 0.3 unanumously 
           for all sensors for now. But we might need its functionality in the future.

Dev logs:
    Jun 19 (Lixing):
        - added compatibility of .iv files 
        - added Sensor class
        - added documentation and bug fixes
    
    Jun 30 (Lixing):
        - added more data 
        - performance optimization to find_threshold
        - implemented RANSAC as an alternative method to fit lines 
        - implemented sub-interval linear interpolation for breakdown estimation
        - bug fixes

    Jul 6 (Lixing):
        - updated breakdown distribution to use histograms of weights, and 
          showed RMSE curve on the same plot
        - depletion voltage estimation with .cv scans
        - switched to new minimum uncertainty: d/sqrt(N), where 
          d is measurement interval, N is # of data pts used
        - bug fixes and readability improvements

    Jul 12 (Lixing):
        - updated breakdown distribution to also include histograms of 
          frequency, to indicate how often a breakdown point is proposed 
          by RANSAC
        - started the design of a new sensor config system that reads and writes 
          information about sensors from and to a common place on the disk
        - reformatted the code structure; moved miscellaneous functions to 
          utils.py. The main script is now called analyze.py.
        - bug fixes; fixed nan issues with calculating mse in ransac() and 
          with plotting rmse with plt.plot

    Jul 13 (Lixing):
        - rebuilt entirely IV analysis code written by Trevor, achieving 
          all original functionalities, removing all hard-coded stuff, 
          greatly improving readability
        - finished config system for sensor. 
        - started design of a secondary config system for scan files that 
          help specify special ways to process certain data, as a replacement 
          to the originally hard-coded method
        - users can now interact with analyze.py via command line arguments
          that allow us to:
            1. analyze only IV scans or CV scans or both
            2. analyze scans of just one or more particular sensors
            3. specify the current type used for IV analysis (pad, gr, total)
            4. clear all plots generated
            5. analyze specific scans by providing paths to scans (not done yet,
            still cooking)
            6. you can view available commands by typing:
                python3 analyze.py --help

    Jul 14 (Lixing):
        - ran the code on IVCV_UNIGE_STRANGE_FEATURES
        - finished the new config system for scans, which comes with a 
          script language I came up with. This system allows us to 1. ignore
          specified bad scans for analysis 2. set particular attributes for 
          analyzing a scan, (depletion volt and ramp type). The language can 
          be expanded to work with more attributes if needed. To edit, go to 
          data_config.txt. Each database has its own data_config.txt at its root.
        - Note: there's not yet a syntax checking for config expressions. 
          be careful.

    Jul 23 (Lixing):
        - added MAX argument to data config system
        - tried sensor-specific baseline MLP model to predict entire IV curve 
          based on temperature alone (DC_W3058 in ./ivcvscans)

    Jul 26 (Lixing) 
        - implemented depletion voltage estimation on IV curve; this algorithm
          is subject to change due to accuracy issues
        - improved from standard RANSAC of using 2 pts for fitting to using 33% 
          of the pts to fit; resolving issue of RANSAC overestimating 
          breakdown in certain cases; improving robustness in noisy data
    
    Jul 27 (Lixing)
        - finished model architecture of an autoencoder and determined the loss 
          function and appropriate latent dimension to use.
        - trained on 144 (unignored) IV scans of DC_W3058 in ./ivcvscans; 
          achieving good reconstruction results with RMSE of 0.0425 on traing 
          data after 292 epochs
        - consolidated Sensor class to untils.py; reformatted the code
        - NumPy style documentation of functions
    
    Jul 28 (Lixing)
        - trained the autoencoder on an entire dataset (ivcvscans) 
          consisting of 402 (unignored) IV scans from 29 sensors,
          achieving good reconstruction results with RMSE of 0.518 on training 
          data after 275 epochs
        - created visualize_latent.py to plot latent space of the ivcvscans 
          dataset, labeled with environmental variables 
        - bug fixes: inf issues in log of current
        - bug fixes: plt not saving plots after plt.show()
        - suppressed numpy invalid value and division by zero err when taking 
          log of leakage current; we removed NaNs and Infs afterwards
        - reformatted README
    
    Jul 29 (Lixing)
        - improved model architecture and loss fxn after many experiment;
          tweaked CNN layers' channel numbers and selectively added LayerNorm;
          added L1 loss component; 
          achiving reconstruction results with RMSE of 0.190 on training
          data after 100 epochs; faster convergence and higher accuracy
        - added an MLP which predicts environmental vars based on 
          latent, in the AE; for semi-supervised learning to nudge more 
          interpreble latent space
        - added spearman correlation and SHAP analysis to correlate 
          environmental variables with latent dimensions
        - added an MLP which predicts latent based on environmental vars,
          used for SHAP analysis
        - bug fixes for running code on windows